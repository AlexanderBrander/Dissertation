{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1PtxGMihN3nqC7JmP6dSPH2AzFNpQNkjV","authorship_tag":"ABX9TyNYyfXFyAFiieSAQAE6E3ga"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PGlTXUeifkoM"},"outputs":[],"source":["import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","source":["base_dir = 'drive/MyDrive/datasetX'\n","\n","train_dir = os.path.join(base_dir, 'train')\n","validation_dir = os.path.join(base_dir, 'val')\n","#auto_test_dir = os.path.join(base_dir, 'auto_test')\n","#test_dir = os.path.join(base_dir, 'test')"],"metadata":{"id":"ITRG1ROMyFi5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_datagen = ImageDataGenerator(\n","    rescale = 1./255,\n","    rotation_range = 40,\n","    width_shift_range = 0.2,\n","    height_shift_range = 0.2,\n","    shear_range = 0.2,\n","    zoom_range = 0.2,\n","    horizontal_flip = True,\n","    fill_mode = 'nearest'\n",")\n","\n","test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_dir,\n","    target_size = (224, 224),\n","    batch_size = 40,\n","    class_mode = 'categorical'\n",")\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_dir,\n","    target_size = (224, 224),\n","    batch_size = 15,\n","    class_mode = 'categorical'\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wWFl27bCzE1p","executionInfo":{"status":"ok","timestamp":1713965947650,"user_tz":-60,"elapsed":3354,"user":{"displayName":"Alexander Brander","userId":"13821396838893011208"}},"outputId":"c10bebcd-e32f-463f-fe27-870a3f42ac3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 940 images belonging to 5 classes.\n","Found 155 images belonging to 5 classes.\n"]}]},{"cell_type":"code","source":["class STNLayer(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(STNLayer, self).__init__(**kwargs)\n","        # Localization net: initially approximates identity transformation\n","        # Matrix [1, 0, 0, 0, 1, 0] reshaped to (2, 3) represents the identity transform\n","        b = np.array([[1, 0, 0], [0, 1, 0]]).astype('float32').flatten()\n","        self.localization_net = tf.keras.Sequential([\n","            layers.Conv2D(8, (7, 7), activation='relu'),\n","            layers.MaxPooling2D(2, 2),\n","            layers.Conv2D(10, (5, 5), activation='relu'),\n","            layers.MaxPooling2D(2, 2),\n","            layers.Flatten(),\n","            layers.Dense(32, activation='relu'),\n","            layers.Dense(6, activation='linear', bias_initializer=tf.keras.initializers.Constant(b))\n","        ])\n","\n","    def call(self, x):\n","        theta = self.localization_net(x)\n","        theta = tf.reshape(theta, (-1, 2, 3))\n","        grid = self._get_grid(x, theta)\n","        transformed_x = self._sample(x, grid)\n","        return transformed_x\n","\n","    def _get_grid(self, x, theta):\n","        batch_size, height, width = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n","        grid = self._meshgrid(height, width)\n","        grid = tf.expand_dims(grid, 0)\n","        grid = tf.tile(grid, [batch_size, 1, 1])\n","        grid_transformed = tf.matmul(theta, grid)\n","        grid_transformed = tf.reshape(grid_transformed, [batch_size, 2, height, width])\n","        grid_transformed = tf.transpose(grid_transformed, [0, 2, 3, 1])\n","        return grid_transformed\n","\n","    def _meshgrid(self, height, width):\n","        x_t = tf.linspace(-1.0, 1.0, width)\n","        y_t = tf.linspace(-1.0, 1.0, height)\n","        x_t, y_t = tf.meshgrid(x_t, y_t)\n","        x_t_flat = tf.reshape(x_t, (1, -1))\n","        y_t_flat = tf.reshape(y_t, (1, -1))\n","        ones = tf.ones_like(x_t_flat)\n","        grid = tf.concat([x_t_flat, y_t_flat, ones], 0)\n","        return grid\n","\n","    def _sample(self, img, grid):\n","        return bilinear_sampler(img, grid[..., 0], grid[..., 1])\n","\n","def bilinear_sampler(img, x, y):\n","\n","    \"\"\"\n","    Performs bilinear sampling of the input image according to the normalized coordinates.\n","\n","    - img: Batch of images in (B, H, W, C) layout.\n","    - x, y: Normalized x, y coordinates of the grid, in (B, H, W) layout.\n","\n","    Returns interpolated images according to grids.\n","    \"\"\"\n","    B = tf.shape(img)[0]\n","    H = tf.shape(img)[1]\n","    W = tf.shape(img)[2]\n","    C = tf.shape(img)[3]\n","    max_y = tf.cast(H - 1, 'int32')\n","    max_x = tf.cast(W - 1, 'int32')\n","    zero = tf.zeros([], dtype='int32')\n","\n","    # Scale x, y from [-1, 1] to [0, W/H - 1]\n","    x = tf.cast((x + 1.0) * tf.cast(W, 'float32') / 2.0, 'float32')\n","    y = tf.cast((y + 1.0) * tf.cast(H, 'float32') / 2.0, 'float32')\n","\n","    # Ensure x, y are within the boundaries\n","    x = tf.clip_by_value(x, 0., tf.cast(max_x, 'float32'))\n","    y = tf.clip_by_value(y, 0., tf.cast(max_y, 'float32'))\n","\n","    # Get pixel value at corner coords\n","    x0 = tf.cast(tf.floor(x), 'int32')\n","    x1 = x0 + 1\n","    y0 = tf.cast(tf.floor(y), 'int32')\n","    y1 = y0 + 1\n","\n","    x0 = tf.clip_by_value(x0, zero, max_x)\n","    x1 = tf.clip_by_value(x1, zero, max_x)\n","    y0 = tf.clip_by_value(y0, zero, max_y)\n","    y1 = tf.clip_by_value(y1, zero, max_y)\n","\n","    # Calculate bilinear interpolation\n","    Ia = get_pixel_value(img, x0, y0)\n","    Ib = get_pixel_value(img, x1, y0)\n","    Ic = get_pixel_value(img, x0, y1)\n","    Id = get_pixel_value(img, x1, y1)\n","\n","    x = tf.cast(x, tf.float32)\n","    y = tf.cast(y, tf.float32)\n","    x0 = tf.cast(x0, tf.float32)\n","    x1 = tf.cast(x1, tf.float32)\n","    y0 = tf.cast(y0, tf.float32)\n","    y1 = tf.cast(y1, tf.float32)\n","\n","    wa = (x1 - x) * (y1 - y)\n","    wb = (x1 - x) * (y - y0)\n","    wc = (x - x0) * (y1 - y)\n","    wd = (x - x0) * (y - y0)\n","    wa = tf.expand_dims(wa, axis=-1)\n","    wb = tf.expand_dims(wb, axis=-1)\n","    wc = tf.expand_dims(wc, axis=-1)\n","    wd = tf.expand_dims(wd, axis=-1)\n","\n","    out = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n","    return out\n","\n","def get_pixel_value(img, x, y):\n","    \"\"\"\n","    Utility function to get pixel values for coordinate\n","    vectors x and y from a 4D tensor image.\n","\n","    img: tensor of shape (B, H, W, C)\n","    x, y: Tensors of shape (B*H*W,), indicating the x and y coordinates respectively.\n","    \"\"\"\n","    shape = tf.shape(img)\n","    B, H, W, C = shape[0], shape[1], shape[2], shape[3]\n","\n","    batch_indices = tf.range(B)\n","    batch_indices = tf.reshape(batch_indices, [B, 1, 1])\n","    batch_indices = tf.tile(batch_indices, [1, H, W])\n","\n","    # Flatten x, y, and batch_indices to use with tf.gather_nd\n","    flat_x = tf.reshape(x, [-1])\n","    flat_y = tf.reshape(y, [-1])\n","    flat_batch_indices = tf.reshape(batch_indices, [-1])\n","\n","    # Stack to create indices for gathering\n","    indices = tf.stack([flat_batch_indices, flat_y, flat_x], axis=1)\n","\n","    # Use tf.gather_nd to gather values from img tensor\n","    result = tf.gather_nd(img, indices)\n","\n","    # Reshape result back to (B, H, W, C) shape\n","    result = tf.reshape(result, [B, H, W, C])\n","    return result\n"],"metadata":{"id":"AYW5EDjC8ydB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Integrating STN into the CNN\n","def build_cnn_with_stn():\n","    inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n","    # Apply STN transformation\n","    transformed_x = STNLayer()(inputs)\n","    # Convolutional layers\n","    x = layers.Conv2D(32, (3, 3), activation='relu')(transformed_x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n","    x = layers.MaxPooling2D(2, 2)(x)\n","    # Dense layers\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(512, activation='relu')(x)\n","    outputs = layers.Dense(5, activation='softmax')(x)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    return model"],"metadata":{"id":"VwUWSzaK82rN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_transformation(original_image, transformed_image):\n","  fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10, 5))\n","\n","  ax1.imshow(original_image)\n","  ax1.set_title(\"Original Image\")\n","  ax1.axis(\"off\")\n","\n","  ax2.imshow(transformed_image)\n","  ax2.set_title(\"Transformed Image\")\n","  ax2.axis(\"off\")\n","\n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"rv8AGlappFCl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = build_cnn_with_stn()\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Training the model\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=15,\n","    epochs=10,\n","    validation_data=validation_generator,\n","    validation_steps=8,\n","    verbose=2\n",")"],"metadata":{"id":"psZXHFLQ9FFB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stn_model = Model(inputs=model.input, outputs=model.get_layer('stn_layer').output)\n","\n","# Visualize the original and transformed images\n","val_images, val_labels = next(iter(validation_generator))\n","transformed_images = stn_model.predict(val_images)\n","predicted_labels = model.predict(val_images)\n","\n","num_examples = 20\n","for i in range(num_examples):\n","    original_image = val_images[i]\n","    transformed_image = transformed_images[i]\n","    true_label = np.argmax(val_labels[i])  # Convert true label to class index\n","    predicted_label = np.argmax(predicted_labels[i])  # Convert predicted label to class index\n","\n","    original_image = (original_image * 255).astype(np.uint8)\n","    transformed_image = (transformed_image * 255).astype(np.uint8)\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n","    ax1.imshow(original_image)\n","    ax1.set_title(f\"Original Image (True Label: {true_label})\")\n","    ax1.axis(\"off\")\n","    ax2.imshow(transformed_image)\n","    ax2.set_title(f\"Transformed Image (Predicted Label: {predicted_label})\")\n","    ax2.axis(\"off\")\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"2clSy1vrD4tN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 5))\n","plt.plot(history.history['loss'], label='Training Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Training and Validation Loss')\n","plt.show()"],"metadata":{"id":"np0if-4IwDJA"},"execution_count":null,"outputs":[]}]}