{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"RxQwLMw7R5z3"},"outputs":[],"source":["# For tips on running notebooks in Google Colab, see\n","# https://pytorch.org/tutorials/beginner/colab\n","%matplotlib inline"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"jr1l9RySixk4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld1D7SgyR5z8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"markdown","metadata":{"id":"oWrYvbrdR5z_"},"source":["Loading the data\n","================\n","\n"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","base_dir = 'drive/MyDrive/datasetL'\n","train_dir = f'{base_dir}/train'\n","test_dir = f'{base_dir}/val'\n","\n","image_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.Grayscale(num_output_channels=1), # Convert images to grayscale\n","        transforms.RandomAffine(degrees=45, translate=(0.8,0.8), scale=(0.5,1.5)),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.Grayscale(num_output_channels=1), # Convert images to grayscale\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.1307,), (0.3081,))\n","    ])\n","}\n","\n","train_dataset = datasets.ImageFolder(root=train_dir, transform=image_transforms['train'])\n","test_dataset = datasets.ImageFolder(root=test_dir, transform=image_transforms['val'])\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=4)"],"metadata":{"id":"DuNrRcADjA0b","executionInfo":{"status":"ok","timestamp":1713906777759,"user_tz":-60,"elapsed":3327,"user":{"displayName":"Alexander Brander","userId":"13821396838893011208"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2ec1dfa-f910-49c1-b14d-ef49cb1b32a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]}]},{"cell_type":"markdown","metadata":{"id":"nDkWVpvUR50C"},"source":["Depicting spatial transformer networks\n","======================================\n","\n","Spatial transformer networks boils down to three main components :\n","\n","-   The localization network is a regular CNN which regresses the\n","    transformation parameters. The transformation is never learned\n","    explicitly from this dataset, instead the network learns\n","    automatically the spatial transformations that enhances the global\n","    accuracy.\n","-   The grid generator generates a grid of coordinates in the input\n","    image corresponding to each pixel from the output image.\n","-   The sampler uses the parameters of the transformation and applies it\n","    to the input image.\n","\n","![](https://pytorch.org/tutorials/_static/img/stn/stn-arch.png)\n","\n","<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n","<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n","<p>We need the latest version of PyTorch that containsaffine_grid and grid_sample modules.</p>\n","</div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRGN0Qe1R50D"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # Adjusting the in_channels for the first convolutional layers to 1 for greyscale images\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5, padding=2)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5, padding=2)\n","        self.conv2_drop = nn.Dropout2d(p=0.5)\n","        self.conv3 = nn.Conv2d(20, 40, kernel_size=5, padding=2)  # Additional Conv layer\n","        self.conv3_drop = nn.Dropout2d(p=0.5)  # Additional dropout layer\n","        self.pool = nn.MaxPool2d(2, stride=2)\n","\n","        # Spatial transformer localisation-network adjustments for greyscale input\n","        self.localization = nn.Sequential(\n","            nn.Conv2d(1, 8, kernel_size=7, padding=3),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.ReLU(True),\n","            nn.Conv2d(8, 10, kernel_size=5, padding=2),\n","            nn.MaxPool2d(2, stride=2),\n","            nn.ReLU(True)\n","        )\n","\n","        # Placeholder for dynamic adjustment of fc_loc and fc1 input sizes\n","        self.fc_loc = nn.Sequential(\n","            nn.Linear(1, 32),  # Placeholder values, will be dynamically adjusted\n","            nn.ReLU(True),\n","            nn.Linear(32, 3 * 2)\n","        )\n","\n","        self.fc1 = nn.Linear(1, 100)  # Placeholder value, will be dynamically adjusted\n","        self.fc2 = nn.Linear(100, 10)\n","\n","        # Initialise the weights/bias with identity transformation\n","        self.fc_loc[2].weight.data.zero_()\n","        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n","\n","        # Placeholder for the dynamically calculated size\n","        self.fc_loc_input_size = None\n","        self.fc1_input_size = None\n","\n","\n","    def stn(self, x):\n","        xs = self.localization(x)\n","        if self.fc_loc_input_size is None:\n","            # Calculates the output size of the localization network dynamically\n","            self.fc_loc_input_size = xs.nelement() // xs.shape[0]\n","            # Assuming we need 32 features before the fully connected layers\n","            self.fc_loc[0] = nn.Linear(self.fc_loc_input_size, 32).to(x.device)\n","        xs = xs.view(-1, self.fc_loc_input_size)\n","        theta = self.fc_loc(xs)\n","        theta = theta.view(-1, 2, 3)\n","\n","        # Batch size is N\n","        N = theta.shape[0]\n","\n","        grid = F.affine_grid(theta, x.size(), align_corners=False)\n","        x = F.grid_sample(x, grid, align_corners=False)\n","        return x\n","\n","\n","    def forward(self, x):\n","        # STN transformation\n","        xs = self.localization(x)\n","        if self.fc_loc_input_size is None:\n","            # Dynamically calculates input size for fc_loc\n","            self.fc_loc_input_size = xs.nelement() // xs.shape[0]  # Total elements divided by batch size\n","            self.fc_loc[0] = nn.Linear(self.fc_loc_input_size, 32).to(x.device)\n","\n","        xs = xs.view(-1, self.fc_loc_input_size)\n","        theta = self.fc_loc(xs)\n","        theta = theta.view(-1, 2, 3)\n","        grid = F.affine_grid(theta, x.size(), align_corners=False)\n","        x = F.grid_sample(x, grid, align_corners=False)\n","\n","        # Normal forward pass\n","        x = F.relu(self.pool(self.conv1(x)))\n","        x = F.relu(self.pool(self.conv2_drop(self.conv2(x))))\n","        x = F.relu(self.pool(self.conv3_drop(self.conv3(x))))\n","\n","        if self.fc1_input_size is None:\n","            # Dynamically calculates input size for fc1\n","            self.fc1_input_size = x.nelement() // x.shape[0]  # Total elements divided by batch size\n","            self.fc1 = nn.Linear(self.fc1_input_size, 100).to(x.device)\n","\n","        x = x.view(-1, self.fc1_input_size)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","\n","        return F.log_softmax(x, dim=1)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Net().to(device)"]},{"cell_type":"markdown","metadata":{"id":"ll7FvNc-R50D"},"source":["Training the model\n","==================\n","\n","Now, let\\'s use the SGD algorithm to train the model. The network is\n","learning the classification task in a supervised way. In the same time\n","the model is learning STN automatically in an end-to-end fashion.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP3OWFNRR50E"},"outputs":[],"source":["optimizer = optim.SGD(model.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2)\n","\n","def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        train_loss += loss.item() * data.size(0)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 500 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item()))\n","    train_loss /= len(train_loader.dataset)\n","    return train_loss\n","#\n","# A simple test procedure to measure the STN performances on MNIST.\n","#\n","\n","\n","def test():\n","    with torch.no_grad():\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","\n","            # sums up batch loss\n","            test_loss += F.nll_loss(output, target, size_average=False).item()\n","            # gets the index of the max log-probability\n","            pred = output.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        test_loss /= len(test_loader.dataset)\n","        accuracy = 100. * correct / len(test_loader.dataset)\n","        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n","              .format(test_loss, correct, len(test_loader.dataset),\n","                      accuracy))\n","    return test_loss"]},{"cell_type":"markdown","metadata":{"id":"As5p0wALR50E"},"source":["Visualizing the STN results\n","===========================\n","\n","Now, we will inspect the results of our learned visual attention\n","mechanism.\n","\n","We define a small helper function in order to visualize the\n","transformations while training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NjwU3OR0R50E"},"outputs":[],"source":["def convert_image_np(inp):\n","    \"\"\"Convert a Tensor to numpy image.\"\"\"\n","    inp = inp.numpy().transpose((1, 2, 0))\n","    mean = np.array([0.485, 0.456, 0.406])\n","    std = np.array([0.229, 0.224, 0.225])\n","    inp = std * inp + mean\n","    inp = np.clip(inp, 0, 1)\n","    return inp\n","\n","# We want to visualize the output of the spatial transformers layer\n","# after the training, we visualise a batch of input images and\n","# the corresponding transformed batch using STN.\n","\n","\n","def visualize_stn():\n","    with torch.no_grad():\n","        # Gets a batch of training data\n","        data = next(iter(test_loader))[0].to(device)\n","\n","        input_tensor = data.cpu()\n","        transformed_input_tensor = model.stn(data).cpu()\n","\n","        in_grid = convert_image_np(\n","            torchvision.utils.make_grid(input_tensor))\n","\n","        out_grid = convert_image_np(\n","            torchvision.utils.make_grid(transformed_input_tensor))\n","\n","        # Plots the results side-by-side\n","        f, axarr = plt.subplots(1, 2, figsize=(20, 10))\n","        axarr[0].imshow(in_grid)\n","        axarr[0].set_title('Dataset Images')\n","\n","        axarr[1].imshow(out_grid)\n","        axarr[1].set_title('Transformed Images')\n","\n","early_stopping_patience = 5\n","best_loss = float('inf')\n","epochs_no_improve = 0\n","num_epochs = 5\n","train_losses = []\n","test_losses = []\n","\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train(epoch)\n","    train_losses.append(train_loss)\n","    test_loss = test()\n","    test_losses.append(test_loss)\n","    # Steps the scheduler with the test loss\n","    scheduler.step(test_loss)\n","\n","    # Checks early stopping conditions\n","    if test_loss < best_loss:\n","      best_loss = test_loss\n","      epochs_no_improve = 0\n","    else:\n","      epochs_no_improve += 1\n","      if epochs_no_improve == early_stopping_patience:\n","        print(\"Early stopping!\")\n","        break"]},{"cell_type":"code","source":["# Visualises the STN transformation on some input batch\n","visualize_stn()\n","\n","plt.ioff()\n","plt.show()"],"metadata":{"id":"NrWWdF_mv-OV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_comparison(model, test_loader, class_names, num_images=2):\n","    # Gets a batch of test data\n","    data, true_labels = next(iter(test_loader))\n","    data = data.to(device)\n","\n","    with torch.no_grad():\n","        # Passes the batch through the network to get the transformed output and predictions\n","        transformed_data = model.stn(data)\n","        output = model(data)\n","        _, predicted_labels = torch.max(output, 1)\n","\n","    # Move data back to CPU for visualization\n","    data = data.cpu().numpy()\n","    transformed_data = transformed_data.cpu().numpy()\n","    true_labels = true_labels.cpu().numpy()\n","    predicted_labels = predicted_labels.cpu().numpy()\n","\n","    # Initialises a plot\n","    fig, axes = plt.subplots(num_images, 2, figsize=(10, 4 * num_images))\n","\n","    for i in range(num_images):\n","        # Gets the original image, true label, transformed image, and predicted label\n","        original_img = data[i].squeeze()\n","        true_label = class_names[true_labels[i]]\n","        transformed_img = transformed_data[i].squeeze()\n","        predicted_label = class_names[predicted_labels[i]]\n","\n","        # Displays original image with true label\n","        ax = axes[i, 0]\n","        ax.imshow(original_img, cmap='gray')\n","        ax.axis('off')\n","        ax.set_title(f'Original Image\\nTrue Label: {true_label}')\n","\n","        # Displays transformed image with predicted label\n","        ax = axes[i, 1]\n","        ax.imshow(transformed_img, cmap='gray')\n","        ax.axis('off')\n","        ax.set_title(f'Transformed Image\\nPredicted Label: {predicted_label}')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Defines the class names (assuming you have 5 classes)\n","class_names = ['Grade 0', 'Grade 1', 'Grade 2', 'Grade 3', 'Grade 4']\n","\n","# Calls the visualization function\n","visualize_comparison(model, test_loader, class_names)"],"metadata":{"id":"HjVuC9ffeCKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training and validation losses plotted\n","plt.figure(figsize=(10, 5))\n","plt.plot(train_losses, label='Training loss')\n","plt.plot(test_losses, label='Validation loss')\n","plt.title('Loss over epochs')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"UROJQIdoGWdB"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/a5513958454950ed22df8da4c47f6429/spatial_transformer_tutorial.ipynb","timestamp":1712493092166}]}},"nbformat":4,"nbformat_minor":0}